{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine import Config\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from get_model import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_root = '07-13-24/modern-model'\n",
    "config = Config.fromfile(os.path.join(ckp_root, 'config.py'))\n",
    "ckp = torch.load(os.path.join(ckp_root, 'last.ckpt'), map_location='cpu')\n",
    "\n",
    "model = get_model(config.model_type, config.model_config)\n",
    "sd = ckp['state_dict'] \n",
    "sd = {k[6:]: v for k, v in sd.items()}\n",
    "model.load_state_dict(sd, strict=False)\n",
    "\n",
    "with open('DATA/word2idx.pkl', 'rb') as f:\n",
    "    word2idx = pickle.load(f)\n",
    "\n",
    "with open('DATA/idx2word.pkl', 'rb') as f:\n",
    "    idx2word = pickle.load(f)\n",
    "model.word2idx = word2idx\n",
    "model.idx2word = idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, \n",
    "            str_strings: str,\n",
    "            device = 'cpu') -> str:\n",
    "    sos_token_idx = model.word2idx['<SOS>']\n",
    "    eos_token_idx = model.word2idx['<EOS>']\n",
    "    decode = lambda x: ''.join([model.idx2word[i] for i in x])\n",
    "    encode = lambda x: [model.word2idx[w] for w in x]\n",
    "    \n",
    "    samples = []\n",
    "    print('sampling...')\n",
    "    inp = torch.tensor([sos_token_idx] + encode(str_strings)).long().to(device)\n",
    "    inp = inp.unsqueeze(0)\n",
    "    for _ in range(50):\n",
    "        out = model(inp)\n",
    "        assert out.shape[0] == 1\n",
    "        next_token_p = torch.nn.functional.softmax(out[0, -1:, :], dim=-1)\n",
    "        \n",
    "        # next_token_idx = torch.argmax(next_token_p, dim=1, keepdim=True)\n",
    "        # do random sample\n",
    "        next_token_idx = torch.multinomial(next_token_p, 1)\n",
    "        inp = torch.cat([inp, next_token_idx], dim=1)\n",
    "        if next_token_idx.item() == eos_token_idx:\n",
    "            break\n",
    "    dec_text = decode(inp.cpu()[0].tolist())\n",
    "    samples.append(dec_text)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<SOS>去年今日此花前|只欠檀羊一笑篇|春色至方无此景|灵根向得最侔旋<EOS>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(model, str_strings='去年今日', device='cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shao-tao-dl-torch-2-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
